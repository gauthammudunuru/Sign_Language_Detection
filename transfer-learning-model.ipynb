{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_name = 'dataset/asl_alphabet_train'\n",
    "trn_dirs = [os.path.join(trn_name, sign) for sign in os.listdir(trn_name)]\n",
    "trn_fnames = [os.listdir(sign_img) for sign_img in trn_dirs]\n",
    "\n",
    "tst_name = 'dataset/asl_alphabet_test'\n",
    "tst_dirs = [os.path.join(tst_name, sign) for sign in os.listdir(tst_name)]\n",
    "tst_fnames = [os.listdir(sign_img) for sign_img in tst_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 29 classes.\n",
      "Found 17400 images belonging to 29 classes.\n",
      "Found 29 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2,\n",
    "    rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(\n",
    "    trn_name,\n",
    "    subset='training',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "vldn_generator = image_generator.flow_from_directory(\n",
    "    trn_name,\n",
    "    subset='validation',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "tst_generator = image_generator.flow_from_directory(\n",
    "    tst_name,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_generator.samples\n",
    "vldn_size = vldn_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 1280)              11837936  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               128100    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 29)                2929      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,968,965\n",
      "Trainable params: 131,029\n",
      "Non-trainable params: 11,837,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_url = \"https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2\"\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SHAPE + (3,)),\n",
    "    tensorflow_hub.KerasLayer(model_url, trainable=False),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(29, activation='softmax',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "\n",
    "model.build((None,) + IMAGE_SHAPE + (3,))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/696 [..............................] - ETA: 13:39 - loss: 3.4074 - acc: 0.0621\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 39/696 [>.............................] - ETA: 13:29 - loss: 3.1832 - acc: 0.1256\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 59/696 [=>............................] - ETA: 13:07 - loss: 2.9659 - acc: 0.1878\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 79/696 [==>...........................] - ETA: 12:48 - loss: 2.7733 - acc: 0.2395\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      " 99/696 [===>..........................] - ETA: 12:23 - loss: 2.5978 - acc: 0.2866\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "119/696 [====>.........................] - ETA: 12:03 - loss: 2.4511 - acc: 0.3207\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "139/696 [====>.........................] - ETA: 11:38 - loss: 2.3200 - acc: 0.3555\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "159/696 [=====>........................] - ETA: 11:13 - loss: 2.2014 - acc: 0.3862\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "179/696 [======>.......................] - ETA: 10:48 - loss: 2.0957 - acc: 0.4143\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "199/696 [=======>......................] - ETA: 10:23 - loss: 2.0046 - acc: 0.4388\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "219/696 [========>.....................] - ETA: 9:58 - loss: 1.9233 - acc: 0.4605\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "239/696 [=========>....................] - ETA: 9:33 - loss: 1.8489 - acc: 0.4798\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "259/696 [==========>...................] - ETA: 9:08 - loss: 1.7822 - acc: 0.4980\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "279/696 [===========>..................] - ETA: 8:43 - loss: 1.7212 - acc: 0.5140\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "299/696 [===========>..................] - ETA: 8:17 - loss: 1.6677 - acc: 0.5282\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "319/696 [============>.................] - ETA: 7:52 - loss: 1.6189 - acc: 0.5414\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "339/696 [=============>................] - ETA: 7:27 - loss: 1.5722 - acc: 0.5542\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "359/696 [==============>...............] - ETA: 7:02 - loss: 1.5293 - acc: 0.5660\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "379/696 [===============>..............] - ETA: 6:36 - loss: 1.4897 - acc: 0.5763\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "399/696 [================>.............] - ETA: 6:11 - loss: 1.4548 - acc: 0.5858\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "419/696 [=================>............] - ETA: 5:46 - loss: 1.4190 - acc: 0.5957\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "439/696 [=================>............] - ETA: 5:21 - loss: 1.3860 - acc: 0.6046\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "459/696 [==================>...........] - ETA: 4:56 - loss: 1.3554 - acc: 0.6132\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "479/696 [===================>..........] - ETA: 4:31 - loss: 1.3273 - acc: 0.6209\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "499/696 [====================>.........] - ETA: 4:06 - loss: 1.3010 - acc: 0.6279\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "519/696 [=====================>........] - ETA: 3:41 - loss: 1.2767 - acc: 0.6347\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "539/696 [======================>.......] - ETA: 3:16 - loss: 1.2531 - acc: 0.6412\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "559/696 [=======================>......] - ETA: 2:51 - loss: 1.2305 - acc: 0.6478\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "579/696 [=======================>......] - ETA: 2:26 - loss: 1.2086 - acc: 0.6539\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "599/696 [========================>.....] - ETA: 2:01 - loss: 1.1865 - acc: 0.6603\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "619/696 [=========================>....] - ETA: 1:36 - loss: 1.1668 - acc: 0.6654\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "639/696 [==========================>...] - ETA: 1:11 - loss: 1.1475 - acc: 0.6710\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "659/696 [===========================>..] - ETA: 46s - loss: 1.1291 - acc: 0.6759\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "679/696 [============================>.] - ETA: 21s - loss: 1.1125 - acc: 0.6805\n",
      "Epoch 00001: saving model to ./checkpoints\\cp.ckpt\n",
      "696/696 [==============================] - 890s 1s/step - loss: 1.0984 - acc: 0.6844 - val_loss: 0.7143 - val_acc: 0.8210\n",
      "INFO:tensorflow:Assets written to: ./model_saves\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_saves\\assets\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./checkpoints') == True:\n",
    "  latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "  model.load_weights(checkpoint_path)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=vldn_generator,\n",
    "    epochs=1,\n",
    "    validation_steps=10,\n",
    "    verbose=1,\n",
    "    callbacks=[cp_callback])\n",
    "\n",
    "model.save('./model_saves')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "test_imgs = [\n",
    "    'dataset/asl_alphabet_train/X/X2.jpg'\n",
    "]\n",
    "\n",
    "# test_imgs = [os.path.join(file, os.listdir(file)[0]) for file in tst_dirs]\n",
    "\n",
    "imgs = [np.expand_dims(\n",
    "    image.img_to_array(\n",
    "        image.load_img(img, target_size=IMAGE_SHAPE)\n",
    "    ), axis=0)\n",
    "    for img in test_imgs]\n",
    "\n",
    "# pass the list of multiple images np.vstack()\n",
    "images = np.vstack(imgs)\n",
    "classes = model.predict(images, batch_size=10)\n",
    "# print the classes, the images belong to\n",
    "all_classes = os.listdir(trn_name)\n",
    "final_results = []\n",
    "for probablities in classes:\n",
    "    final_results.append(all_classes[np.argmax(probablities)]) \n",
    "# final_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n"
     ]
    }
   ],
   "source": [
    "def predict_asl(frame) -> str:\n",
    "\tframe = image.smart_resize(frame, (224, 224))\n",
    "\timg = np.expand_dims(image.img_to_array(frame), axis=0)\n",
    "\tpredicted_asl = model.predict(img)\n",
    "\treturn all_classes[np.argmax(predicted_asl)]\n",
    "\n",
    "alphabet = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_number = 0\n",
    "while(True):\n",
    "\tret, frame = cap.read()\n",
    "\tcv2.putText(frame,\n",
    "             alphabet,\n",
    "             (50, 50),\n",
    "             font, 1,\n",
    "             (0, 255, 255),\n",
    "             2,\n",
    "             cv2.LINE_4)\n",
    "\tcv2.imshow(\"Webcam\", frame)\n",
    "\twhile (frame_number == 180):\n",
    "\t\talphabet = predict_asl(frame)\n",
    "\t\tprint(alphabet)\n",
    "\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\tframe_number = 0\n",
    "\tframe_number += 1\n",
    "\n",
    "\n",
    "\tch = cv2.waitKey(1)\n",
    "\tif ch & 0xFF == ord('q'):\n",
    "\t\tcap.release()\n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c4337cb28130a45e22977257afd0fd0a7cba64c2a30280441f9418d3d63af4d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
